[{"content":"The Secret Sauce Behind PageRank Algorithm\n1. Abstract What happens when you search for a term on your browser? Google\u0026rsquo;s PageRank algorithm has revolutionized how the world accesses data on the web. The computation involved in ordering search results entails the indexing of trillions of pages and the creation of high dimensional structures to store and calculate different probabilities. This article provides a general overview of how the algorithm works by briefly exploring the general theory behind PageRank. In addition to explaining the mathematical foundations of PageRank, this article will present a simulation meant to help the reader gain some intuition of the stochastic nature of this algorithm. Also, it looks at some of the caveats including how PageRank deals with dangling links.\nRandom surfer on the web\n2. Introduction In the 1998 paper Bringing Order to the Web, Sergey Brin and Larry Page describe PageRank as a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. Simply put, the importance of a page can be thought of in terms of the probability that a random walk through the web will end up on a given webpage. This algorithm captures the global ranking of all web pages based on their location in the world wide web\u0026rsquo;s graph structure.\n3. Theory The underpinnings of PageRank are based on the hyperlink structure of different pages on the web. The links pointing to a page are considered as a recommendation from the page containing the outbound link. Inlinks from good pages (pages with a higher rank) carry more weight than those from less important pages. Each webpage is assigned an appropriate score which helps determine the importance and authority of the page. The importance of any page is increased by the number and quality of sites which link to it. The following equation represents the rank of any given page $P$:\n$r(P) = \\sum _ { Q \\in B _ { P } } \\frac { r ( Q ) } { | Q | }$ where $B_P$ = all pages pointing to $P$ and $|Q|$ = number of outlinks from $Q$.\n3.1 Computation 3.1.1 Markov model Given x connected webpages, how would we rank them in order of importance? As mentioned earlier, this process constitutes a random walk on a graph representing different pages on the web. The walk’s position at any fixed time only depends on the last vertex (webpage) visited and not on the previous locations of the walk. Any time we are at a given node of the graph, we choose an edge (hyperlink) uniformly at random to determine the node to visit at the next step. This sequence maintains a transition matrix M whose columns contain the series of probability vectors $\\vec { x } _ { 0 } , \\vec { x } _ { 1 } , \\vec { x } _ { 2 } , \\dots$. The transition probabilities and the transition matrix for a random walk through this system are defined as:\n$M _ { i j } = \\left\\{ \\begin{array} { l l } { P \\left( X _ { 1 } = j | X _ { 0 } = i \\right) = 1 / \\operatorname { link } ( i ) , } \u0026 { \\text { if } i \\vec { \\sim } j } \\\\ { 0 , } \u0026 { \\text { otherwise } } \\end{array} \\right.$ Where $i \\vec { \\sim } j$ represents a directed edge from page i to page j and link$(i)$ represents the number of directed edges from i. The random surfer process can be visualized in a matrix containing probabilities that show the likelihoods of moving from one page to every other page. The matrix is row normalized with nonzero elements of row i corresponding to the outlinking pages of page i. The nonzero elements of column i correspond to the inlinking pages of page i.\n$M = \\left[ \\begin{array} { c c c c c } { 0 } \u0026 { 0 } \u0026 { 1 / 2 } \u0026 { 0 } \u0026 { 1 / 2 } \\\\ { 0 } \u0026 { 0 } \u0026 { 1 } \u0026 { 0 } \u0026 { 0 } \\\\ { 1 / 4 } \u0026 { 1 / 4 } \u0026 { 0 } \u0026 { 1 / 4 } \u0026 { 1 / 4 } \\\\ { 0 } \u0026 { 0 } \u0026 { 1 / 2 } \u0026 { 0 } \u0026 { 1 / 2 } \\\\ { 0 } \u0026 { 0 } \u0026 { 0 } \u0026 { 0 } \u0026 { 1 } \\end{array} \\right]$ These initial ranks are successively updated by adding up the weights of every page that link to them divided by the number of links emanating from the referring page. If a page has no outlinks, its rank is equally redistributed to the other pages in the graph. This redistribution is applied to all the pages in the graph until the ranks stabilize. Computing this distribution is the equivalent of taking the limiting distribution of the chain containing all the ranks.\n3.1.2 Damping factor In practice, the PageRank algorithm adds a damping factor $d$ at each stage to prevent pages with no outgoing links from affecting the PageRanks of pages connected to them. This heuristic relfects the probability that a random surfer on the web may choose to randomly \u0026ldquo;jump\u0026rdquo; from one node to another in the web graph. This may happen when he decides to type another page\u0026rsquo;s address directly into the search bar on the browser after hitting a dead end. The damping factor ranges from 0 - 1 where 0 corresponds to successive random jumps between different pages and 1 to a sequence of random successive clicks that inevitably lead to a page with no outgoing links. The sum of weighted PageRanks of all pages is multiplied by this value.\n3.2 Simulating PageRank In this section, R code that simulates PageRank is presented. It begins by generating a random n by n adjacency matrix containing the link structure between different pairs of pages in an arbitrary web system and proceeds as follows:\n# Define function to generate an adjacency matrix A_gen \u003c- function (n, d) { A \u003c- matrix(sample(0:1, n*n, prob = c(1-d, d), replace = T), ncol = n, byrow = T) return(A) } The adjacency matrix generated above and the damping factor $d$ are used as inputs to the page_rank() function defined below which calculates the ranks of all the vertices in the graph. The process starts with the initialization of a transition matrix and proceeds to iteratively calculate the rank of all pages until the Markov chain converges. # Define PageRank function (p = probability of teleport) PageRank \u003c- function (A, p, output = T) { # Define assertions if (!is.matrix(A) | !all(A %in% 0:1)) { stop(noquote('no (valid) adjacency matrix is provided.')) } else if (!is.numeric(p) | p \u003c 0 | p \u003e 1) { stop(noquote('p must be a probability between 0 and 1.')) } # Initialize transition matrix s \u003c- matrix(rep(NA, ncol(A)), ncol = ncol(A)) s[1, ] \u003c- rep(1/ncol(A), ncol(A)) i \u003c- 1 # Repeat Markov Chain until convergence while (T) { # Calculate transition vector at t + 1 t \u003c- rep(NA, ncol(A)) for (j in 1:ncol(A)) { t[j] \u003c- ifelse(sum(A[j, ]) == 0 , 1 / ncol(A) , p / ncol(A) + (1 - p) * sum(A[, j] * (s[i, ] / apply(A, 1, sum))))} s \u003c- rbind(s, t) i \u003c- i + 1 # Break if converged if (i \u003e 1) if (all(round(s[i - 1, ], 4) == round(s[i, ], 4))) break } # Build and return output rank \u003c- data.frame(as.character(1:ncol(s)), round(s[i, ], 4), rep(p, ncol(A))) colnames(rank) \u003c- c('Node', 'PageRank', 'P') if (output) { cat(noquote('PageRank Output:\\n\\n')) print(rank[order(-rank$PageRank), c('Node', 'PageRank')], row.names = F) cat(noquote(paste('\\nPageRank converged in', i, 'iterations.'))) } else { return(rank[order(-rank$PageRank), ]) } } # Generate graph set.seed(327) A \u003c- A_gen(10, .6) The process modeled above is ran on a graph with 10 vertices and r sum(A) edges. A visual representation of how different nodes in the graph are connected is also generated. Below it, is output showing the page ranks of all 10 nodes. # Plot the graph plot(graph.adjacency(A,'directed')) # Run PageRank on example graph above with d = 0.2 PageRank(A, .2) ## ## Node: PageRank ## 2 0.1419\t## 1 0.1143\t## 3 0.1103\t## 9\t0.1078 ## 10 0.1056 ## 7 0.0995 ## 6 0.0916 ## 4\t0.0904\t## 8 0.0803 ## 5 0.0582 3. Limitations The original paper on PageRank by Brin et al (1998) presents dangling links on the web as a pertinent issue affecting the model. It decribes them as links that point to pages with no outgoing links and states that they affect the model because it is not clear where their weights should be distributed. The ultimate proposition made in the paper is the removal of all these links until all the PageRanks are calculated. Brin and Page postulate that the addition of these links thereafter doesn\u0026rsquo;t affect things significantly.\n4. Conclusion PageRank is a powerful tool that has greatly simplified and democratized the access to information on the web. The advent of this algorithm has seen the emergence of a Search Engine Optimization industry which is a discipline focused on organically increasing the reputation of online content to increase visibility and improve rankings on the web. However, one of the major challenges this poses to PageRank is that commercial entities may try to artificially game the system to increase the visibility of their online content.\n5. References Langville, Amy N., and Carl D. Meyer. “The Mathematics of Google’s PageRank.” Google\u0026rsquo;s PageRank and Beyond: The Science of Search Engine Rankings, Princeton University Press, 2006, pp. 31–46. JSTOR, www.jstor.org/stable/j.ctt7t8z9.7.\nBenincasa, Catherine \u0026amp; Calden, Adena \u0026amp; Hanlon, Emily \u0026amp; Kindzerske, Matthew \u0026amp; Law, Kody \u0026amp; Lam, Eddery \u0026amp; Rhoades, John \u0026amp; Roy, Ishani \u0026amp; Satz, Michael \u0026amp; Valentine, Eric \u0026amp; Whitaker, N. (2018). Page Rank Algorithm.\nDobrow, R. P.. Probability : With Applications and R, John Wiley \u0026amp; Sons, Incorporated, 2013. ProQuest Ebook Central, https://ebookcentral.proquest.com/lib/amherst/detail.action?docID=1449975.\nSergey Brin, and Larry Page. Bringing Order to the Web.\nHwai-Hui Fu, et al. “APPLIED STOCHASTIC MODELS IN BUSINESS AND INDUSTRY.” Damping Factor in Google Page Ranking, 29 June 2006, pp. 1–14.\nhttps://en.wikipedia.org/wiki/PageRank\n","permalink":"https://nzaba.github.io/posts/pagerank/","summary":"\u003cp\u003eThe Secret Sauce Behind PageRank Algorithm\u003c/p\u003e","title":"Markov Chains Demystified"},{"content":"Harnessing KNN \u0026amp; Neural Nets to Assess Default Risk for Unbanked Loan Applicants\nIntroduction Lending money is a profitable enterprise but only if you’re loaning to people that pay back their loans. Therefore, it is very important for banks and companies to be able to assess beforehand whether or not a person is likely to repay a loan. Many criteria have been considered over the years in attempts to gauge the responsibility of a borrower including but not limited to the borrower’s previous loan history, the area in which they are living, and how long it has been since they changed phone numbers. One of the key metrics used in the credit history, a measure of how you have behaved with previous loans calculated from how much you borrow, how often you make late payments, and how often you default. Most lending entities today prioritize credit history as their metric of choice when considering loan applications and often will not loan to applicants who cannot present a satisfactory credit history. However, this leads to a problem for both the loaners and the borrowers: the borrowers cannot get the capital they might need without submitting to terrible loan deals and the loaners leave a large population of customers untapped. By analyzing nonstandard metrics other than credit history to predict the trustworthiness of a borrower, this analysis seeks to bridge this gap. An accurate classification model could provide the loan entities with some assurance that their loans will be repaid and that, with less risk, they would be more willing to loan to this non borrowing population, benefiting everyone.\nTo address this issue, an initial exploratory factor analysis (EFA) is conducted, in which I attempt to recover latent variables relating to clients’ trustworthiness. The hope is that such observations will be informative to future attempts at classification and in the compiling of other data sets. In other words, by finding which variables in our data set are most informative, companies and researchers can prioritize them both at the data collection stage and at the evaluation stage.\nAfter this, a k-nearest neighbors classifier is applied to the data to form a predictive model. This model, if accurate, would be a major deliverable of the project and ready for use by loan entities. At the very least this attempt at classification will inform us as to whether or not the quantitative variables being collected and used are informative to the question of interest.\nA logistic neural net is also applied (with the same rationale as the k-nearest neighbor model). It’s chosen because we are interested in seeing whether we can do a better job in predicting whether or not a client will default on a loan. A neural net is a platform for many machine learning techniques. A system of neuron nodes are set up each with a different random weights and connected to a random selection of the input variables. The process runs on the training set to classify the data and then adjusts the weights of its neurons based on the most erroneous cases, repeating until the weights of the neurons are no longer adjusting or until some other specified cutoff point is reached. By using both of these classification methods, we will compare the two models and either confirm the accuracy or our predictions or indicate more work is needed on one model or the other.\nData The data used comes from the Home Credit Group which is interested in giving loans to clients that have no credit history. As such, other metrics must be used and the challenge of this dataset is evaluating a borrower’s likelihood of repaying from these other criteria. The 307511 observation dataset of loans contains 122 alternative variables and is not terribly tidy. Some extra work is needed before working with the dataset. Firstly, a uniform random sample of the dataset is taken to cut it down to 10,000 observations. We then pare down the dataset’s variables by eliminating any with less than 90% complete observations. This leaves us with 66 variables, most of them indicator flags of whether an applicant met this or that criteria. Upon some further trimming, we decide to use the following nine quantitative variables:\nTarget: binary indicator of default, 0 is no default, 1 is default.\nIncome: annual income of the borrower\nLoan amount: the dollar amount of the requested loan\nLoan annuity: how much collateral the borrower could present\nRegion Population: the normalized level of population for that area\nClient Age: age in years\nLength of Employment: negative values indicate unemployment\nNumber of Family Members\nDays Since Last Phone Change\nExtraction We query an SQLite database to obtain client information using the DBI package. The dbConnect() function is used to create a database connection. Invoking it returns an object that can be used to communicate with the database management system (DBMS), providing access to dynamic queries, results and management sessions.\n## Target Income Amount Annuity Pop Age Emp Fam Phone ## 1 0 157500 288873 14805.0 0.022625 -10676 -1190 2 -2 ## 2 0 207000 781920 34573.5 0.046220 -21762 365243 1 -882 ## 3 0 103500 161730 11385.0 0.024610 -17623 -1636 3 -2293 ## 4 0 405000 1800000 47614.5 0.010006 -13362 -232 4 -309 ## 5 0 157500 545040 25537.5 0.025164 -13115 -6211 1 0 ## 6 0 157500 314055 17167.5 0.003122 -16330 -1971 3 -808 The table above shows the first six client entries after obtaining the 9 variables of interest.\nExploratory Analysis A correlation matrix is constructed from the quantitative variables to get a closer look at correlation values as well as the distributions of the variables in our dataset. Most of the correlations were weak. The only strong correlation observed was between loan annuity and loan amount which makes sense since we would expect the bank to be willing to loan more money if more collateral is presented. Most of the variables are right skewed. It is also interesting to see the presence of multiple distinct peaks among the different variable distributions.\nMethods Experiments In addition to being listed on this report, the experiments conducted in this study are shared and maintained on Tableau Public. Feel free to check out my published worksheet below!\nFactor Analysis A Factor Analysis is more appropriate in this analysis than Principal Components Analysis because we are interested in seeing which latent indicators of trustworthiness / ability to pay back loans can be recovered from a set of manifest variables. A Maximum Likelihood Approach is used to determine, through repeated hypothesis testing, which number of factors is most appropriate.\nClassification K Nearest Neighbors:\nKNN classifiers with 10 values of k (1-10) are fitted and represented on plots showing the apparent and estimated error rates for the various values of k. An 80% ~ 20% holdout sample approach is used here to obtain the respective train and test sets. Neural Nets:\nThe second classification method used was neural nets. Neural nets are composed of layers of weighted neurons that pass on 0 or 1 depending on whether the weighted sums of their inputs exceed their activation potential. The system is modeled after the function of the neurons in the human brain. By working on the training set iteratively, the weights of the neurons can be refined based on the misclassifications until all training samples are classified (or, to reduce over fitting, until a certain number of iterations have been reached). By layering these sets of neurons and having the outputs of one layer be fed into the next, you can do this weight-refining approach several times and achieve accuracy in complex classifications far beyond the scope of regression, as evidenced by their use in photo analysis, voice identification, and of course in our own heads. In this case, a holdout sample approach was used to evaluate the sufficiency of the neural network model. The proportions chosen were 75% ~ 25% for the train and test set respectively. The caret package is used to find the best parameters to use for this classifier. To kick start the process, we set up a grid of tuning parameters for the model, fitted each and calculated a bootstrapped AUC (Area under ROC curve) score for every hyper parameter combination. The values with the biggest AUC are chosen for the neural network. The preferred evaluation metric is AER and estimated TER, but the only available ones are: sensitivity, specificity, area under the ROC and AUC. It is worth noting that all available predictors are used to fit the model. The final parameters chosen are: three interconnected neuron layers with a decay rate of 0.5 (factor by which each weight is multiplied by after each update.)1. The weighted inputs and bias are summed to form the net input to the next layer. The inputs are mapped from layer to layer then finally fed into a sigmoid function that outputs the desired probability that a client will default.\nResults Factor Analysis Five possible factor solutions are examined to find one with the most compelling interpretation. Although the p-values yielded from the maximum likelihood evaluation of the five factor solution suggested that none of them could adequately account for the variations in the data, the factors in the five factor model satisfy the principles of Thurstone’s simple structure. The first factor in this model loads highly on credit and annuity thus it could be labelled non-Income assets. The second one is a contrast between age and duration of employment. The third is dominated by continuous family members therefore it could be seen as a measure of client mobility. The fourth reflects income whereas the fifth seems like a weighted average.\n#Call: factanal(x = train_sample, factors = 5) Uniquenesses:\nIncome Amount Credit Amount Annuity Region Pop Days Birth Days Employed Days Registration Days ID Publish Fam Members Last Phone Change 0.394 0.005 0.350 0.947 0.342 0.165 0.760 0.782 0.675 0.929 Loadings:\nFactor 1 Factor 2 Factor 3 Factor 4 Factor 5 Income Amount 0.218 0.106 0.735 Credit Amount 0.979 0.175 Annuity 0.716 0.356 Region Pop 0.206 Days Birth 0.600 0.496 0.219 Days Employed -0.841 -0.300 0.185 Days Registration 0.179 0.377 0.253 Days ID Publish 0.392 0.248 Fam Members 0.546 -0.116 Last Phone Change 0.258 Factor 1 Factor 2 Factor 3 Factor 4 Factor 5 SS loadings 1.532 1.263 0.802 0.753 0.301 Proportion Var 0.153 0.126 0.080 0.075 0.030 Cumulative Var 0.153 0.280 0.360 0.435 0.465 K Nearest Neighbors Figure1 is the graph of the AER and TER for the KNN runs on values of K from 1:10. Larger values of k perform fairly well. Additionally, we see a convergence toward a 0.08 error rate in both the AER and the TER. However, due to the nature of our data, we shouldn’t treat all error rates equally. From the viewpoint of the company, mistakenly classifying a paying borrower as nonpaying is less harmful than classifying a nonpaying borrower as paying. In other words, a false positive from our classifier means we give a loan that is not paid back. Let’s consider the error rate of only these harmful errors in Figure2.\nWe can see in Figure2 that the AER behaves essentially identically to the run with both error rates while the TER is markedly different in the beginning but then both continue to approach 0.08. This suggests that, as we increase k, we eliminate the Type I errors of our model on new data. Unfortunately the harmful Type II error rate is not eliminated and remains at 0.08. This means that we should expect 8% of the applicants given the go-ahead by our model to actually not repay their loans. On the upside however, if our model indicates an applicant cannot be trusted, it is almost always correct.\nNeural Networks The results obtained from the grid search process show that the prediction accuracy increases as a function of the number of units in the hidden layer of the neural network. There is also a discernible improvement in model quality for moderate weight decay values. The estimated true error rate was 8.64% and the apparent error rate was 8.87%. Both of these error rates are based on a majority class prediction.\nConclusions From the results obtained from this analysis, we can reasonably recommend the adaptation of a KNN classifier due to its robust performance in predicting default risk. Although a neural network performs similarly, it proved to be an extremely slow learner that takes long to run. More data and resamples would have been brought in given more time and resources to see whether the observed issues with multivariate normality can be fixed. It is also important to note that the scope of inference from the methods used in this analysis are only applicable to individuals with similar histories to those in this study.\nCitations Brownlee, Jason. “Tuning Machine Learning Models Using the Caret R Package.” Machine Learning Mastery, 22 Sept. 2016, www.machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/. Portilla, Jose. “KDnuggets.” KDnuggets Analytics Big Data Data Mining and Data Science, 2016, www.kdnuggets.com/2016/08/begineers-guide-neural-networks-r.html. “Weight Decay in Neural Networks.” Metacademy, metacademy.org/graphs/concepts/weight_decay_neural_networks. R documentation caret package Multivariate Analysis With R Wickham, H. (n.d.). R Database Interface (DBI 1.1.0). Retrieved December 11, 2020, from https://dbi.r-dbi.org/ ","permalink":"https://nzaba.github.io/posts/defaultriskcalculations/","summary":"\u003cp\u003eHarnessing KNN \u0026amp; Neural Nets to Assess Default Risk for Unbanked Loan Applicants\u003c/p\u003e","title":"Default Risk Estimation Methods"},{"content":"Forecasting Methods for Modelling Changing Variance in Time Series\nAbstract This project is based on exploring ARCH /GARCH methods and their application in modelling the changing variance of time in predicting stock prices. The data used in this study is obtained from the S\u0026amp;P 500 index, which is a measure that estimates the performance of 500 companies listed in the United States Stock exchange market. The data used includes daily data spanning the years 2013 - 2018.\nIntroduction - ARIMA models The first phase of this analysis begins with the exploration of an ARIMA, where ARIMA stands for Auto Regressive Moving Average models. They consist of two components; the Autoregressive Component and the Moving Average Component and are denoted as ARIMA(p,d,q), with p representing the number of autoregressive terms, d the number of differencing and q the number of Moving Average terms. The second phase involves the checking of model residuals (to look for volatile clusters) followed by an eventual transition to ARCH/GARCH.\nBefore we begin any model fitting, we make the line graphs shown below. This step is meant as an initial exploration aimed at showing the trends in volume and price of Chesapeake Energy Corporation’s stocks. All the plots below are similar because they show that the volume, opening, high and low values exhibited high volatility. Volatility in a time series refers to the phenomenon where the conditional variance of a time series varies over time (Cryer and Chan, 2008). Stock volume seems more volatile than low, high and close prices, as can be seen from the more sudden irregular shifts in trends with time.\nThe data wrangling required before fitting the model/ checking conditions is minimal. It begins by decomposing Chesapeake monthly data time series into seasonal, trend and irregular components, then moves on to the removal of seasonal components to create a seasonally adjusted component.\nModel Conditons Before fitting an ARIMA time series model, we need to make sure that it is free of trends and non seasonal behavior. We also check to make sure that the time series has a constant mean and variance. If variation in trends is present, we difference in order to get rid of those trends and prepare the data for model fitting. After all these checks are performed, we run the Augmented Dickey Fuller test to make sure that stationarity is satisfied:\nThe hypothesis test to check whether our data is stationary is as follows:\n$H_0$: Chesapeake time series is not stationary. $H_A$: Chesapeake time series is stationary. Our test yields a statistic of −3.5423 and a p value of 0.03823. We therefore reject the null as there is strong evidence of stationarity in the data, and proceed to the model fitting phase.\n#adf test checks whether ts is stationary or not (data condition) adf.test(CHK_deseasonal_value, alternative = \"stationary\") ## ## Augmented Dickey-Fuller Test ## ## data: CHK_deseasonal_value ## Dickey-Fuller = -3.5423, Lag order = 10, p-value = 0.03823 ## alternative hypothesis: stationary Model Fitting Here, we use the Auto.arima() function to help obtain model parameters using a stepwise model fitting procedure. This model selection procedure selects the model with the lowest AIC value. The p, d, q parameters of the model will be selected from the model with the lowest score. We start with a maximum order of 6 for all parameters and iterate through different combinations to find one that produces the model with the lowest AIC score:\n#auto fits arima model CHK_fit = auto.arima(CHK_deseasonal_value, max.order = 6) CHK_fit ## Series: CHK_deseasonal_value ## ARIMA(4,1,3) ## ## Coefficients: ## ar1 ar2 ar3 ar4 ma1 ma2 ma3 ## 0.2097 0.3550 0.6572 -0.4885 0.2163 -0.3094 -0.7638 ## s.e. 0.0449 0.0418 0.0359 0.0263 0.0468 0.0452 0.0373 ## ## sigma^2 estimated as 1.864e+12: log likelihood=-19460.3 ## AIC=38936.59 AICc=38936.71 BIC=38977.65 The model chosen from our selection procedure has 4 Autoregressive Terms i.e AR(4), a differencing of degree 1 and 3 moving average terms i.e MA(3). The fitted model from the parameters obtained above can be expressed as :\n$$\\hat{Y}_{t} = 0.2097 Y_{t-1}+0.3550 Y_{t-2}+0.6572 Y_{t-3}-0.4885 Y_{t-4}+0.2163 e_{t-1}-0.3094 e_{t-2}-0.7638 e_{t-3}+\\epsilon$$ The equation above is a linear combination of terms. The Y’s correspond to recent stock volume values up until the $(t-4)^{th}$ time step while the $e$\u0026rsquo;s correspond to the errors of the lags at the denoted, corresponding time steps.\nIn the next section we shall examine sample partial autocorrelation (PACF) and sample autocorrelation plots (ACF) to validate our choices of the p, d and q orders chosen for our ARIMA model by the stepwise model selection procedure.\nModel Diagnostics Before settling down on the model obtained from the previous section, we examine the auto correlation of residuals to make sure our model is free of auto correlations. This is necessary because it helps us establish whether the noise terms in the model are independent from each other:\nggtsdisplay(residuals(CHK_fit), plot.type = \"partial\", main = \"ARIMA (4, 1, 3) Diagnostic plots\") A quick examination of the ACF plot reveals the existence of excessive correlations in the residuals at lags 3 and 6. Furthermore, the lag patterns in the PACF are quite similar to those in the ACF plot, suggesting the existence of autocorrelation. This problem in distribution of error terms also manifests itself in the residuals, and can be seen from the unequal variation of error terms across the range of values in the residual plot.\nIn order to confirm the findings from the diagnostic plots above, we conduct an official hypothesis test for model fit using the Ljung - Box test to see whether the error terms in the model are independently and identically distributed (i.i.d).\nThe Ljung Box test statistic is given by (Glen, 2018): $$Q_* = n(n+2) \\sum_{k=1}^{m} \\frac{r_{k}^{2}}{n-k}$$ where n is the size of the time series and r the residual correlation at the $k^{th}$ lag . $Q_*$ has a chi-square distribution with k-p-q degrees of freedom (Cryer and Chan, 2008). The official hypothesis for the test are as follows:\n$H_0$ : The model error terms are uncorrelated\n$H_A$ : The model error terms are correlated\nBox.test(residuals(CHK_fit), lag = 90, fitdf = 83, type = 'Ljung-Box') ## ## Box-Ljung test ## ## data: residuals(CHK_fit) ## X-squared = 541, df = 7, p-value \u003c 2.2e-16 Running the test yields a p value of 2.2e-16. We have sufficient evidence to reject the null, as there is strong evidence that the model assumption of independence of error terms has been violated. In the next section we attempt to find the remedy to this problem by exploring methods that model the changing variance in time series.\nARCH/ GARCH models Introduction In the previous chapter we tried fitting and assessing the feasibility of an ARIMA model on Chesapeake Energy Corporation’s stock data. The fitted model wasn’t appropriate because the model’s error terms were not independent and identically distributed. In addition, cluster volatility seemed to be a huge issue as observed from the heteroschedastic nature of modeled stock volume returns.\nIn this section we will use autoregressive conditional heteroschedastic models in an attempt to adequately capture and account for the heteroscedasticity observed in the ARIMA model. ARCH/GARCH are time series models used to model processes where volatility is high in provided data (Cryer and Chan, 2008). Cases involving stock market data are usually prone to unpredictable changes, and are best modeled using methods that model the variability of future values based on present and past provided trends in observed returns.\n1. ARCH models ARCH models are denoted ARCH(p), where p represents the order of the model. According to Cryer and Chan (2008) an ARCH(1) process modelling the return of a time series r takes the form $r_{t}=\\sigma_{t | t-1} \\varepsilon_{t}$ where $\\varepsilon_{t}$ is a series of independent and identically distributed random variables with a mean of zero and standard deviation of 1. The quantity $\\sigma_{t | t-1}$ models the conditional variance, $\\sigma_{t | t-1}^{2}$, of the return $r_{t}$, and is given by $\\sigma_{t | t-1}^{2}=\\omega+\\alpha r_{t-1}^{2}$. The variance of the current return is based on conditioning upon returns until the ${(t-1)}^{th}$ time step. The quantities $\\omega$ and $\\alpha$ represent the ARCH model intercept and coefficient respectively.\nThe diagram below represents a sample ARCH(1) process simulated with $\\omega$ = 0.1 and $\\alpha$ = 0.5 as the chosen model parameters.\nset.seed(85) garch01.sim = garch.sim(alpha=c(.1,.5),n=400) sim_data \u003c- data.frame(seq(1,400),garch01.sim ) arch.plt \u003c- ggplot(sim_data, aes(x = seq.1..400., y = garch01.sim )) + geom_line() + xlab(\"Time Step\") + ylab(\"Return at Time Step t\") + ggtitle(\"Simulated ARCH(1) process\") ggplotly(arch.plt) 2. GARCH models The ARCH model introduced in the previous section models future returns by conditioning the value of the variance at time t to the previous time step alone i.e $\\sigma_{t | t-1}^{2}=\\omega+\\alpha r_{t-1}^{2}$. Bollerslev\u0026rsquo;s (1986) approach encourages the backward extension of this conditioning process up until the $q^{th}$ time step as well as the introduction of p lags to the conditional variance (Cryer and Chan, 2008). This resulting model becomes a Generalized Autoregressive Conditional Heteroscedasticity (GARCH) process, and is denoted as GARCH(p,q). The return from this new proposed model takes the same form as ARCH\u0026rsquo;s $r_{t}=\\sigma_{t | t-1} \\varepsilon_{t}$. However, the conditional variance $\\sigma^2_{t | t-1}$ modeled by the quantity $\\sigma_{t | t-1}$ now becomes : $$\\begin{aligned} \\sigma_{t | t-1}^{2}=\\omega+\\beta_{1} \\sigma_{t-1 | t-2}^{2}+\\cdots+\u0026amp; \\beta_{p} \\sigma_{t-p | t-p-1}^{2}+\\alpha_{1} r_{t-1}^{2}+\\alpha_{2} r_{t-2}^{2}+\\cdots+\\alpha_{q} r_{t-q}^{2} \\end{aligned}$$\nThe $\\beta$ coefficients in the model are used to assign weights to the lags of the conditioned variance values.\nThe plot below (Cryer and Chan, 2008) illustrates an example of a simulated GARCH(1,1) process with parameter values $$\\omega=0.02, \\alpha=0.05, \\text { and } \\beta = 0.9$$\nThe parameters $\\omega, \\beta, \\alpha$ in GARCH and $\\omega, \\alpha$ in ARCH are constrained to $\u0026gt;0$, since the conditional variances have to be positive.\nEstimation of GARCH model coefficents GARCH model coefficients are fit using the Maximum Likelihood Approach. The estimation process used to obtain the likelihood estimates for $\\omega, \\beta, \\alpha$ is based on recursively iterating through the log likelihood function modelling the GARCH coefficient estimates. The log likelihood function we aim to maximize is defined as (Cryer and Chan, 2008):\n$$L(\\omega, \\alpha, \\beta)=-\\frac{n}{2} \\log (2 \\pi)-\\frac{1}{2} \\sum_{i=1}^{n}\\left\\{\\log \\left(\\sigma_{t-1 | t-2}^{2}\\right)+r_{t}^{2} / \\sigma_{t | t-1}^{2}\\right\\}$$ Model Fitting Below, we try fitting an appropriate GARCH model using the ugarchfit() function from the rugarch package. The package computes the model estimates using the maximum likelihood function specified in the previous section.\nWe will explore different GARCH orders, with the aim of finding the model that best fits the data. The orders we will try are arbitrarily chosen as GARCH(1,1), GARCH(2,2)and GARCH(3,3):\nModel Diagnostics The GARCH(1,1) model seems like the most appropriate here since all but one of it\u0026rsquo;s model coefficients are significant. Furthermore, it has the lowest AIC of all the models explored.\nBefore we accept the model we found in the previous section we need to make sure that assumptions have been met for the ARCH(1,1) model. The squared residuals need to be serially uncorrelated and the error terms should be normally distributed:\n#QQplot to check for normality qqnorm(residuals(fit_mod1)); qqline(residuals(fit_mod1)) #Sample acf and pacf plots to check iid assumption ggtsdisplay(residuals(fit_mod1), plot.type = \"partial\", main = \"Chesapeak Energy Corporation's GARCH (1, 1) Diagnostic plots\") From the diagnostic plots made above, we see a major problem with normality, as most of the points on the qqplot veer off the line. In addition, there seems to be major issues with independence, as can be seen from the significant lags in residuals from the autocorrelation plots.\nResults From the model diagnostics we ran in the previous section, we discovered major issues with normality and independence in error terms. We might want to explore more model parameters to see whether we can find a model that improves upon what we currently have. For now, we will proceed with extreme caution and use the GARCH(1,1) model to make some predictions.\nBelow, we extract the coefficients from the chosen GARCH(1,1) model.\n#Extract coefficients fit_mod1@fit$matcoef ## Estimate Std. Error t value Pr(\u003e|t|) ## mu 16.96828513 0.03046360 557.001998 0.000000e+00 ## omega 0.04592897 0.01059138 4.336448 1.448039e-05 ## alpha1 0.40407479 0.05760516 7.014559 2.306821e-12 ## beta1 0.51414297 0.06573577 7.821358 5.329071e-15 The return at time t as given by our model is going to be given by (Boudt, 2020) : $$R_{t} = 16.97 +e_{t}$$ where the $e_{t}$ is a normally distributed random variable with a mean of 0 and variance of $\\widehat{\\sigma}_{t}^{2}$ i.e $e_{t} \\sim N\\left(0, \\widehat{\\sigma}_{t}^{2}\\right)$. The variance modelling return volatility at the $t^{th}$ time step in our fitted model is going to be $\\widehat{\\sigma}_{t}^{2}=0.05+0.40 e_{t-1}^{2}+0.51 \\widehat{\\sigma}_{t-1}^{2}$ The ugarchroll() function is used to obtain estimates for the last four dates in the data set. The test data that is used is the most recent week\u0026rsquo;s returns in stock volume. The log volume residuals obtained after this process are printed down below :\n#Forecasting using the ugarchforecast function preds = ugarchforecast(fit_mod1, n.ahead = 1, data = CHK[1255:1259, ,drop = F][,6]) preds = ugarchroll(spec = spec_mod1 , data = log(CHK_ts) , n.start = 1255 , refit.every = 2 , refit.window = 'moving') References Cryer, J. D., \u0026amp; Chan, K.-sik. (2008). Time series analysis with applications in R. New York: Springer. Nugent, C. (n.d.). S\u0026amp;P 500 stock data. Retrieved from https://www.kaggle.com/camnugent/sandp500. Boudt, K. (n.d.). GARCH Models in R. Retrieved from https://www.datacamp.com/courses/garch-models-in-r. Trapletti, A., \u0026amp; Hornik, K. (n.d.). Package ‘tseries.’ Retrieved from https://cran.r-project.org/web/packages/tseries/tseries.pdf Ghalanos, A., \u0026amp; Kley, T. (n.d.). Package ‘rugarch.’ Retrieved from https://cran.r-project.org/web/packages/rugarch/rugarch.pdf Glen, S. (2018, September 11). Ljung Box Test: Definition. Retrieved from https://www.statisticshowto.datasciencecentral.com/ljung-box-test/ Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31(3), 307–327. doi: 10.1016/0304-4076(86)90063-1 ","permalink":"https://nzaba.github.io/posts/variancemodelling/","summary":"\u003cp\u003eForecasting Methods for Modelling Changing Variance in Time Series\u003c/p\u003e","title":"Modelling Changing Variance in Time Series"},{"content":"A Shiny App Which Sheds More Light on the Opioid Crisis\n","permalink":"https://nzaba.github.io/posts/opioidcrisis/","summary":"\u003cp\u003eA Shiny App Which Sheds More Light on the Opioid Crisis\u003c/p\u003e","title":"Opioid Crisis Dashboard"}]